{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "classes = (\"yes\", \"no\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (224, 224)\n",
    "b_size = 16\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.RandomResizedCrop(size=img_size, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        #transforms.RandomRotation(30),\n",
    "        transforms.RandomApply([transforms.RandomAffine(degrees=(-15, 15), translate=(0.1, 0.1),\n",
    "                                        scale=(0.9, 1.1))], p=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(\"../dataset/resnet/train\", transform=transform_test)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=b_size, num_workers=0, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(\"../dataset/resnet/test\", transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=b_size, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Curob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Curob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PEP_Classifier(\n",
       "  (base_model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Dropout(p=0.7, inplace=False)\n",
       "      (1): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PEP_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PEP_Classifier, self).__init__()    \n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        in_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Linear(in_features, 2)\n",
    "        )\n",
    "        \n",
    "        # Инициализация весов последнего слоя\n",
    "        nn.init.xavier_uniform_(self.base_model.fc[1].weight)\n",
    "        nn.init.zeros_(self.base_model.fc[1].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Убираем sigmoid - используем CrossEntropyLoss с logits\n",
    "        return self.base_model(x)\n",
    "    \n",
    "model = PEP_Classifier()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.0001, weight_decay=0.0005, momentum=0.9)\n",
    "scheduler1 = CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.9348, Train Accuracy: 0.6792\n",
      "Val Loss: 0.3924, Val Accuracy: 0.7857\n",
      "Epoch [2/100], Loss: 0.2781, Train Accuracy: 0.8858\n",
      "Val Loss: 0.3769, Val Accuracy: 0.8036\n",
      "Epoch [3/100], Loss: 0.3052, Train Accuracy: 0.8955\n",
      "Val Loss: 0.2259, Val Accuracy: 0.8482\n",
      "Epoch [4/100], Loss: 0.1593, Train Accuracy: 0.9405\n",
      "Val Loss: 0.2772, Val Accuracy: 0.8482\n",
      "Epoch [5/100], Loss: 0.1831, Train Accuracy: 0.9380\n",
      "Val Loss: 0.3460, Val Accuracy: 0.8214\n",
      "Epoch [6/100], Loss: 0.1456, Train Accuracy: 0.9490\n",
      "Val Loss: 0.1115, Val Accuracy: 0.9464\n",
      "Epoch [7/100], Loss: 0.1028, Train Accuracy: 0.9575\n",
      "Val Loss: 0.2052, Val Accuracy: 0.8750\n",
      "Epoch [8/100], Loss: 0.0822, Train Accuracy: 0.9660\n",
      "Val Loss: 0.2433, Val Accuracy: 0.8750\n",
      "Epoch [9/100], Loss: 0.1197, Train Accuracy: 0.9526\n",
      "Val Loss: 0.1363, Val Accuracy: 0.9107\n",
      "Epoch [10/100], Loss: 0.0768, Train Accuracy: 0.9769\n",
      "Val Loss: 0.2068, Val Accuracy: 0.8839\n",
      "Epoch [11/100], Loss: 0.0839, Train Accuracy: 0.9745\n",
      "Val Loss: 0.2135, Val Accuracy: 0.8929\n",
      "Epoch [12/100], Loss: 0.0699, Train Accuracy: 0.9684\n",
      "Val Loss: 0.0975, Val Accuracy: 0.9375\n",
      "Epoch [13/100], Loss: 0.0217, Train Accuracy: 0.9915\n",
      "Val Loss: 0.1397, Val Accuracy: 0.9107\n",
      "Epoch [14/100], Loss: 0.0503, Train Accuracy: 0.9830\n",
      "Val Loss: 0.1687, Val Accuracy: 0.9018\n",
      "Epoch [15/100], Loss: 0.0614, Train Accuracy: 0.9757\n",
      "Val Loss: 0.1023, Val Accuracy: 0.9286\n",
      "Epoch [16/100], Loss: 0.0449, Train Accuracy: 0.9891\n",
      "Val Loss: 0.0853, Val Accuracy: 0.9464\n",
      "Epoch [17/100], Loss: 0.0450, Train Accuracy: 0.9842\n",
      "Val Loss: 0.1364, Val Accuracy: 0.9196\n",
      "Epoch [18/100], Loss: 0.0449, Train Accuracy: 0.9830\n",
      "Val Loss: 0.1329, Val Accuracy: 0.9196\n",
      "Epoch [19/100], Loss: 0.0349, Train Accuracy: 0.9854\n",
      "Val Loss: 0.0866, Val Accuracy: 0.9554\n",
      "Epoch [20/100], Loss: 0.0315, Train Accuracy: 0.9866\n",
      "Val Loss: 0.0457, Val Accuracy: 0.9911\n",
      "Epoch [21/100], Loss: 0.0192, Train Accuracy: 0.9951\n",
      "Val Loss: 0.0714, Val Accuracy: 0.9643\n",
      "Epoch [22/100], Loss: 0.0280, Train Accuracy: 0.9927\n",
      "Val Loss: 0.0547, Val Accuracy: 0.9911\n",
      "Epoch [23/100], Loss: 0.0103, Train Accuracy: 0.9964\n",
      "Val Loss: 0.0824, Val Accuracy: 0.9554\n",
      "Epoch [24/100], Loss: 0.0174, Train Accuracy: 0.9915\n",
      "Val Loss: 0.0425, Val Accuracy: 0.9911\n",
      "Epoch [25/100], Loss: 0.0224, Train Accuracy: 0.9939\n",
      "Val Loss: 0.0555, Val Accuracy: 0.9821\n",
      "Epoch [26/100], Loss: 0.0128, Train Accuracy: 0.9951\n",
      "Val Loss: 0.1221, Val Accuracy: 0.9286\n",
      "Epoch [27/100], Loss: 0.0105, Train Accuracy: 0.9964\n",
      "Val Loss: 0.1038, Val Accuracy: 0.9286\n",
      "Epoch [28/100], Loss: 0.0105, Train Accuracy: 0.9951\n",
      "Val Loss: 0.0638, Val Accuracy: 1.0000\n",
      "Epoch [29/100], Loss: 0.0193, Train Accuracy: 0.9939\n",
      "Val Loss: 0.0869, Val Accuracy: 0.9464\n",
      "Epoch [30/100], Loss: 0.0135, Train Accuracy: 0.9964\n",
      "Val Loss: 0.0822, Val Accuracy: 0.9375\n",
      "Epoch [31/100], Loss: 0.0832, Train Accuracy: 0.9781\n",
      "Val Loss: 0.1543, Val Accuracy: 0.9196\n",
      "Epoch [32/100], Loss: 0.0302, Train Accuracy: 0.9927\n",
      "Val Loss: 0.1749, Val Accuracy: 0.9107\n",
      "Epoch [33/100], Loss: 0.0201, Train Accuracy: 0.9939\n",
      "Val Loss: 0.0633, Val Accuracy: 0.9732\n",
      "Epoch [34/100], Loss: 0.0185, Train Accuracy: 0.9927\n",
      "Val Loss: 0.0875, Val Accuracy: 0.9286\n",
      "Epoch [35/100], Loss: 0.0436, Train Accuracy: 0.9818\n",
      "Val Loss: 0.1364, Val Accuracy: 0.9107\n",
      "Epoch [36/100], Loss: 0.0273, Train Accuracy: 0.9891\n",
      "Val Loss: 0.1458, Val Accuracy: 0.9107\n",
      "Epoch [37/100], Loss: 0.0220, Train Accuracy: 0.9915\n",
      "Val Loss: 0.0663, Val Accuracy: 0.9643\n",
      "Epoch [38/100], Loss: 0.0841, Train Accuracy: 0.9854\n",
      "Val Loss: 0.0511, Val Accuracy: 0.9821\n",
      "Epoch [39/100], Loss: 0.0061, Train Accuracy: 0.9976\n",
      "Val Loss: 0.1407, Val Accuracy: 0.9286\n",
      "Epoch [40/100], Loss: 0.0078, Train Accuracy: 0.9976\n",
      "Val Loss: 0.1834, Val Accuracy: 0.9107\n",
      "Epoch [41/100], Loss: 0.0171, Train Accuracy: 0.9927\n",
      "Val Loss: 0.0582, Val Accuracy: 0.9821\n",
      "Epoch [42/100], Loss: 0.0198, Train Accuracy: 0.9903\n",
      "Val Loss: 0.1362, Val Accuracy: 0.9286\n",
      "Epoch [43/100], Loss: 0.0088, Train Accuracy: 0.9976\n",
      "Val Loss: 0.1818, Val Accuracy: 0.9107\n",
      "Epoch [44/100], Loss: 0.0200, Train Accuracy: 0.9927\n",
      "Val Loss: 0.0720, Val Accuracy: 0.9643\n",
      "Epoch [45/100], Loss: 0.0138, Train Accuracy: 0.9964\n",
      "Val Loss: 0.0855, Val Accuracy: 0.9375\n",
      "Epoch [46/100], Loss: 0.0200, Train Accuracy: 0.9939\n",
      "Val Loss: 0.0387, Val Accuracy: 1.0000\n",
      "Epoch [47/100], Loss: 0.0182, Train Accuracy: 0.9951\n",
      "Val Loss: 0.1853, Val Accuracy: 0.9196\n",
      "Epoch [48/100], Loss: 0.0089, Train Accuracy: 0.9976\n",
      "Val Loss: 0.0463, Val Accuracy: 1.0000\n",
      "Epoch [49/100], Loss: 0.0100, Train Accuracy: 0.9964\n",
      "Val Loss: 0.0991, Val Accuracy: 0.9375\n",
      "Epoch [50/100], Loss: 0.0140, Train Accuracy: 0.9964\n",
      "Val Loss: 0.0610, Val Accuracy: 0.9732\n",
      "Epoch [51/100], Loss: 0.0095, Train Accuracy: 0.9939\n",
      "Val Loss: 0.0514, Val Accuracy: 0.9821\n",
      "Epoch [52/100], Loss: 0.0048, Train Accuracy: 1.0000\n",
      "Val Loss: 0.0418, Val Accuracy: 1.0000\n",
      "Epoch [53/100], Loss: 0.0143, Train Accuracy: 0.9939\n",
      "Val Loss: 0.0514, Val Accuracy: 0.9732\n",
      "Epoch [54/100], Loss: 0.0541, Train Accuracy: 0.9891\n",
      "Val Loss: 0.0367, Val Accuracy: 1.0000\n",
      "Epoch [55/100], Loss: 0.0155, Train Accuracy: 0.9976\n",
      "Val Loss: 0.0662, Val Accuracy: 0.9643\n",
      "Epoch [56/100], Loss: 0.0507, Train Accuracy: 0.9781\n",
      "Val Loss: 0.0724, Val Accuracy: 0.9643\n",
      "Epoch [57/100], Loss: 0.0194, Train Accuracy: 0.9939\n",
      "Val Loss: 0.0848, Val Accuracy: 0.9554\n",
      "Epoch [58/100], Loss: 0.0237, Train Accuracy: 0.9927\n",
      "Val Loss: 0.0649, Val Accuracy: 0.9643\n",
      "Epoch [59/100], Loss: 0.0176, Train Accuracy: 0.9951\n",
      "Val Loss: 0.0972, Val Accuracy: 0.9375\n",
      "Epoch [60/100], Loss: 0.0081, Train Accuracy: 0.9976\n",
      "Val Loss: 0.0350, Val Accuracy: 1.0000\n",
      "Epoch [61/100], Loss: 0.0055, Train Accuracy: 0.9988\n",
      "Val Loss: 0.0753, Val Accuracy: 0.9464\n",
      "Epoch [62/100], Loss: 0.0043, Train Accuracy: 0.9988\n",
      "Val Loss: 0.0402, Val Accuracy: 0.9911\n",
      "Epoch [63/100], Loss: 0.0215, Train Accuracy: 0.9915\n",
      "Val Loss: 0.0375, Val Accuracy: 1.0000\n",
      "Epoch [64/100], Loss: 0.0221, Train Accuracy: 0.9915\n",
      "Val Loss: 0.0785, Val Accuracy: 0.9554\n",
      "Epoch [65/100], Loss: 0.0167, Train Accuracy: 0.9964\n",
      "Val Loss: 0.0378, Val Accuracy: 1.0000\n",
      "Epoch [66/100], Loss: 0.0096, Train Accuracy: 0.9964\n",
      "Val Loss: 0.1164, Val Accuracy: 0.9286\n",
      "Epoch [67/100], Loss: 0.0126, Train Accuracy: 0.9951\n",
      "Val Loss: 0.1157, Val Accuracy: 0.9286\n",
      "Epoch [68/100], Loss: 0.0090, Train Accuracy: 0.9976\n",
      "Val Loss: 0.0783, Val Accuracy: 0.9554\n",
      "Epoch [69/100], Loss: 0.0081, Train Accuracy: 0.9976\n",
      "Val Loss: 0.0726, Val Accuracy: 0.9554\n",
      "Epoch [70/100], Loss: 0.0048, Train Accuracy: 0.9988\n",
      "Val Loss: 0.0574, Val Accuracy: 0.9643\n",
      "Epoch [71/100], Loss: 0.0167, Train Accuracy: 0.9927\n",
      "Val Loss: 0.0626, Val Accuracy: 0.9643\n",
      "Epoch [72/100], Loss: 0.0412, Train Accuracy: 0.9964\n",
      "Val Loss: 0.0237, Val Accuracy: 1.0000\n",
      "Epoch [73/100], Loss: 0.0104, Train Accuracy: 0.9951\n",
      "Val Loss: 0.0911, Val Accuracy: 0.9286\n",
      "Epoch [74/100], Loss: 0.0063, Train Accuracy: 0.9976\n",
      "Val Loss: 0.0313, Val Accuracy: 1.0000\n",
      "Epoch [75/100], Loss: 0.0219, Train Accuracy: 0.9939\n",
      "Val Loss: 0.0214, Val Accuracy: 1.0000\n",
      "Epoch [76/100], Loss: 0.0392, Train Accuracy: 0.9866\n",
      "Val Loss: 0.0168, Val Accuracy: 1.0000\n",
      "Epoch [77/100], Loss: 0.0048, Train Accuracy: 0.9988\n",
      "Val Loss: 0.0478, Val Accuracy: 0.9643\n",
      "Epoch [78/100], Loss: 0.0067, Train Accuracy: 0.9976\n",
      "Val Loss: 0.0331, Val Accuracy: 1.0000\n",
      "Epoch [79/100], Loss: 0.0102, Train Accuracy: 0.9951\n",
      "Val Loss: 0.0715, Val Accuracy: 0.9643\n",
      "Epoch [80/100], Loss: 0.0273, Train Accuracy: 0.9915\n",
      "Val Loss: 0.0415, Val Accuracy: 0.9911\n",
      "Epoch [81/100], Loss: 0.0055, Train Accuracy: 0.9988\n",
      "Val Loss: 0.0676, Val Accuracy: 0.9643\n",
      "Epoch [82/100], Loss: 0.0157, Train Accuracy: 0.9939\n",
      "Val Loss: 0.0464, Val Accuracy: 0.9732\n",
      "Epoch [83/100], Loss: 0.0020, Train Accuracy: 1.0000\n",
      "Val Loss: 0.0760, Val Accuracy: 0.9643\n",
      "Epoch [84/100], Loss: 0.0145, Train Accuracy: 0.9927\n",
      "Val Loss: 0.0608, Val Accuracy: 0.9643\n",
      "Epoch [85/100], Loss: 0.0130, Train Accuracy: 0.9976\n",
      "Val Loss: 0.0368, Val Accuracy: 1.0000\n",
      "Epoch [86/100], Loss: 0.0293, Train Accuracy: 0.9927\n",
      "Val Loss: 0.0347, Val Accuracy: 1.0000\n",
      "Epoch [87/100], Loss: 0.0125, Train Accuracy: 0.9951\n",
      "Val Loss: 0.0315, Val Accuracy: 1.0000\n",
      "Epoch [88/100], Loss: 0.0037, Train Accuracy: 0.9988\n",
      "Val Loss: 0.0384, Val Accuracy: 1.0000\n",
      "Epoch [89/100], Loss: 0.0328, Train Accuracy: 0.9915\n",
      "Val Loss: 0.0389, Val Accuracy: 1.0000\n",
      "Epoch [90/100], Loss: 0.0081, Train Accuracy: 0.9964\n",
      "Val Loss: 0.0309, Val Accuracy: 1.0000\n",
      "Epoch [91/100], Loss: 0.0073, Train Accuracy: 0.9976\n",
      "Val Loss: 0.0339, Val Accuracy: 1.0000\n",
      "Epoch [92/100], Loss: 0.0109, Train Accuracy: 0.9976\n",
      "Val Loss: 0.0779, Val Accuracy: 0.9554\n",
      "Epoch [93/100], Loss: 0.0150, Train Accuracy: 0.9939\n",
      "Val Loss: 0.0233, Val Accuracy: 1.0000\n",
      "Epoch [94/100], Loss: 0.0063, Train Accuracy: 0.9976\n",
      "Val Loss: 0.0501, Val Accuracy: 0.9732\n",
      "Epoch [95/100], Loss: 0.0060, Train Accuracy: 0.9976\n",
      "Val Loss: 0.0755, Val Accuracy: 0.9643\n",
      "Epoch [96/100], Loss: 0.0056, Train Accuracy: 0.9988\n",
      "Val Loss: 0.0510, Val Accuracy: 0.9732\n",
      "Epoch [97/100], Loss: 0.0134, Train Accuracy: 0.9964\n",
      "Val Loss: 0.0142, Val Accuracy: 1.0000\n",
      "Epoch [98/100], Loss: 0.0156, Train Accuracy: 0.9951\n",
      "Val Loss: 0.0819, Val Accuracy: 0.9464\n",
      "Epoch [99/100], Loss: 0.0063, Train Accuracy: 0.9976\n",
      "Val Loss: 0.0441, Val Accuracy: 0.9821\n",
      "Epoch [100/100], Loss: 0.0157, Train Accuracy: 0.9951\n",
      "Val Loss: 0.0679, Val Accuracy: 0.9643\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "max_accuracy = 0\n",
    "#count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "            \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler2.step()\n",
    "        \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {(train_loss/len(trainloader)):.4f}, Train Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    writer.add_scalar(\"Train accuracy\", accuracy, epoch + 1)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    for images, labels in testloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f'Val Loss: {(val_loss/len(testloader)):.4f}, Val Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    if (accuracy > max_accuracy):\n",
    "        max_accuracy = accuracy\n",
    "    \n",
    "    scheduler1.step()\n",
    "    \n",
    "    writer.add_scalar(\"Val loss\", val_loss/len(testloader), epoch + 1)\n",
    "    writer.add_scalar(\"Val accuracy\", accuracy, epoch + 1)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    writer.add_scalar(\"Lerning rate\", current_lr, epoch + 1)\n",
    "\n",
    "print(max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
